# è§†é¢‘åŠ¨ä½œåˆ†ç±»è®­ç»ƒæ–‡æ¡£

æœ¬ç›®å½•åŒ…å«äº†ä½¿ç”¨ `copilot/migrate-video-action-classification` åˆ†æ”¯çš„ä¿®æ”¹è¿›è¡Œè§†é¢‘åŠ¨ä½œåˆ†ç±»è®­ç»ƒçš„å®Œæ•´æ–‡æ¡£å’Œç¤ºä¾‹ã€‚

## ğŸ“– ä¸»è¦æ–‡æ¡£

- **[è§†é¢‘åŠ¨ä½œåˆ†ç±»è”åˆè®­ç»ƒæŒ‡å¯¼æ–‡æ¡£](video_action_classification_guide.md)** - å®Œæ•´çš„è®­ç»ƒæŒ‡å—

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### æ–°å¢çš„ `action_cls` è®­ç»ƒé˜¶æ®µ

æœ¬æ¬¡ä¿®æ”¹å¼•å…¥äº†ä¸“é—¨ç”¨äºè§†é¢‘åŠ¨ä½œåˆ†ç±»çš„è®­ç»ƒé˜¶æ®µï¼Œä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š

1. **ActionDecoder åˆ†ç±»å¤´** (`LlamaFactory/src/llamafactory/model/action_decoder.py`)
   - æ”¯æŒ `linear` å’Œ `mlp` ä¸¤ç§æ¶æ„
   - è½»é‡çº§è®¾è®¡ï¼Œæ˜“äºè®­ç»ƒå’Œéƒ¨ç½²
   - æ”¯æŒä¿å­˜å’ŒåŠ è½½é¢„è®­ç»ƒæƒé‡

2. **ç‰¹æ®Š Token `<ACT>`**
   - åœ¨è¾“å…¥åºåˆ—ä¸­æ ‡è®°åŠ¨ä½œåˆ†ç±»ä½ç½®
   - ç³»ç»Ÿè‡ªåŠ¨æå–è¯¥ token çš„éšè—çŠ¶æ€ç”¨äºåˆ†ç±»

3. **è”åˆè®­ç»ƒæœºåˆ¶**
   - åŒæ—¶ä¼˜åŒ–è§†è§‰-è¯­è¨€ä¸»å¹²ï¼ˆé€šè¿‡ LoRAï¼‰å’Œåˆ†ç±»å¤´
   - ç«¯åˆ°ç«¯çš„è®­ç»ƒæµç¨‹
   - æ”¯æŒå¤šæ•°æ®é›†æ··åˆè®­ç»ƒ

## ğŸ“ æ–‡ä»¶ç»“æ„

```
docs/
â”œâ”€â”€ README.md                                    # æœ¬æ–‡ä»¶
â””â”€â”€ video_action_classification_guide.md        # è¯¦ç»†æŒ‡å¯¼æ–‡æ¡£

LlamaFactory/
â”œâ”€â”€ src/llamafactory/
â”‚   â”œâ”€â”€ model/action_decoder.py                  # ActionDecoder å®ç°
â”‚   â”œâ”€â”€ data/processor/action_cls.py             # æ•°æ®å¤„ç†å™¨
â”‚   â””â”€â”€ train/action_cls/                        # è®­ç»ƒæ¨¡å—
â”‚       â”œâ”€â”€ trainer.py                           # è‡ªå®šä¹‰ Trainer
â”‚       â””â”€â”€ workflow.py                          # è®­ç»ƒå·¥ä½œæµ
â”œâ”€â”€ tests/train/test_action_cls.py               # å•å…ƒæµ‹è¯•
â””â”€â”€ examples/train_lora_action_cls/              # é…ç½®ç¤ºä¾‹
    â”œâ”€â”€ qwen2_5vl_ucf101.yaml                    # UCF101 é…ç½®
    â””â”€â”€ qwen2_5vl_sthsthv2.yaml                  # Something-Something é…ç½®
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®

å‚è€ƒæŒ‡å¯¼æ–‡æ¡£ä¸­çš„ [æ•°æ®é›†å‡†å¤‡](video_action_classification_guide.md#æ•°æ®é›†å‡†å¤‡) ç« èŠ‚ï¼Œå‡†å¤‡ UCF101 æˆ– Something-Something V2 æ•°æ®é›†ã€‚

### 2. é…ç½®è®­ç»ƒ

ä½¿ç”¨æä¾›çš„ç¤ºä¾‹é…ç½®æ–‡ä»¶æˆ–æ ¹æ®éœ€è¦è‡ªå®šä¹‰ï¼š

```bash
# ç¼–è¾‘é…ç½®æ–‡ä»¶
vi LlamaFactory/examples/train_lora_action_cls/qwen2_5vl_ucf101.yaml

# å…³é”®å‚æ•°ï¼š
# - model_name_or_path: Qwen2.5-VL-3B-Instruct æ¨¡å‹è·¯å¾„
# - num_action_classes: åŠ¨ä½œç±»åˆ«æ•°ï¼ˆUCF101=101, SthSthV2=174ï¼‰
# - dataset: æ•°æ®é›†åç§°
```

### 3. å¯åŠ¨è®­ç»ƒ

```bash
cd LlamaFactory

# å• GPU è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=0
llamafactory-cli train examples/train_lora_action_cls/qwen2_5vl_ucf101.yaml

# å¤š GPU è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=0,1,2,3
torchrun --nproc_per_node 4 --master_port 29500 \
    -m llamafactory.cli train \
    examples/train_lora_action_cls/qwen2_5vl_ucf101.yaml
```

## ğŸ“Š æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | ç±»åˆ«æ•° | è®­ç»ƒæ ·æœ¬ | é…ç½®æ–‡ä»¶ |
|--------|--------|----------|----------|
| UCF101 | 101 | ~9,537 | `qwen2_5vl_ucf101.yaml` |
| Something-Something V2 | 174 | ~168k | `qwen2_5vl_sthsthv2.yaml` |

## ğŸ’¡ æ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®éœ€è¦éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š

```json
{
  "messages": [
    {
      "content": "<video>What action is being performed in this video?",
      "role": "user"
    },
    {
      "content": "The action being performed is <ACT>.",
      "role": "assistant"
    }
  ],
  "videos": ["/path/to/video.mp4"],
  "action_label": 5
}
```

**å…³é”®è¦ç‚¹**ï¼š
- åŠ©æ‰‹å›å¤ä¸­å¿…é¡»åŒ…å« `<ACT>` token
- `action_label` å¿…é¡»æ˜¯æ•´æ•°ï¼ˆä» 0 å¼€å§‹ï¼‰
- `videos` åŒ…å«è§†é¢‘æ–‡ä»¶çš„è·¯å¾„

## ğŸ”§ ä¸»è¦å‚æ•°è¯´æ˜

### è®­ç»ƒé˜¶æ®µå‚æ•°

```yaml
stage: action_cls  # å¿…é¡»è®¾ç½®ä¸º action_cls
```

### åŠ¨ä½œåˆ†ç±»å‚æ•°

```yaml
num_action_classes: 101              # åŠ¨ä½œç±»åˆ«æ€»æ•°
action_decoder_type: linear          # decoder ç±»å‹: linear æˆ– mlp
action_decoder_hidden_size: null     # mlp æ¨¡å¼ä¸‹çš„éšè—å±‚ç»´åº¦
action_decoder_path: null            # é¢„è®­ç»ƒ decoder è·¯å¾„ï¼ˆå¯é€‰ï¼‰
action_token_lr_scale: 0.1           # <ACT> token å­¦ä¹ ç‡ç¼©æ”¾å› å­
```

### LoRA å‚æ•°

```yaml
finetuning_type: lora
lora_rank: 16                        # LoRA ç§©
lora_alpha: 32                       # LoRA alpha
lora_target: all                     # åº”ç”¨ LoRA çš„ç›®æ ‡æ¨¡å—
use_dora: true                       # ä½¿ç”¨ DoRAï¼ˆæ¨èï¼‰
```

## ğŸ“ ç¤ºä¾‹è„šæœ¬

### æ•°æ®å‡†å¤‡è„šæœ¬

åœ¨ `scripts/` ç›®å½•ä¸‹åˆ›å»ºæ•°æ®è½¬æ¢è„šæœ¬ï¼š

- `prepare_ucf101.py` - UCF101 æ•°æ®é›†è½¬æ¢
- `prepare_sthsthv2.py` - Something-Something V2 æ•°æ®é›†è½¬æ¢

è¯¦ç»†ä»£ç å‚è§[æŒ‡å¯¼æ–‡æ¡£](video_action_classification_guide.md#æ•°æ®é›†å‡†å¤‡)ã€‚

### è®­ç»ƒè„šæœ¬

```bash
#!/bin/bash
export CUDA_VISIBLE_DEVICES=0,1,2,3
export WANDB_PROJECT=video-action-classification

cd /path/to/IVCP/LlamaFactory

torchrun --nproc_per_node 4 --master_port 29500 \
    -m llamafactory.cli train \
    examples/train_lora_action_cls/qwen2_5vl_ucf101.yaml
```

## ğŸ› æ•…éšœæ’æŸ¥

å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼š

1. **æ˜¾å­˜ä¸è¶³**ï¼šå‡å° `per_device_train_batch_size` å’Œ `video_max_pixels`
2. **<ACT> token æœªæ‰¾åˆ°**ï¼šæ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œç¡®ä¿åŠ©æ‰‹å›å¤ä¸­åŒ…å« `<ACT>` token
3. **æŸå¤±ä¸ä¸‹é™**ï¼šè°ƒæ•´å­¦ä¹ ç‡ï¼Œæ£€æŸ¥æ ‡ç­¾èŒƒå›´
4. **æ•°æ®åŠ è½½æ…¢**ï¼šå¢åŠ  `preprocessing_num_workers` å’Œ `dataloader_num_workers`

æ›´å¤šè¯¦æƒ…å‚è§[æ•…éšœæ’æŸ¥ç« èŠ‚](video_action_classification_guide.md#æ•…éšœæ’æŸ¥)ã€‚

## ğŸ“š å‚è€ƒèµ„æº

- [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)
- [Qwen2.5-VL æ¨¡å‹](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- [UCF101 æ•°æ®é›†](https://www.crcv.ucf.edu/data/UCF101.php)
- [Something-Something V2](https://developer.qualcomm.com/software/ai-datasets/something-something)

## âš™ï¸ ç³»ç»Ÿè¦æ±‚

- **GPU**: NVIDIA GPU with CUDA 11.8+ (æ¨è A100/V100, è‡³å°‘ 24GB æ˜¾å­˜)
- **Python**: 3.9+
- **å­˜å‚¨**: è‡³å°‘ 500GB

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª Apache License 2.0ã€‚

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** è¯·æŸ¥çœ‹[å®Œæ•´æŒ‡å¯¼æ–‡æ¡£](video_action_classification_guide.md)æˆ–æäº¤ Issueã€‚
