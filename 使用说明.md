# IVCP 使用说明文档

## 最近修改内容

### 视频动作分类训练阶段 (action_cls)

本次更新添加了一个新的训练阶段 `action_cls`，用于视频动作分类任务。该功能通过特殊的 `<ACTION>` 标记(token)实现视频动作识别。

#### 核心功能

1. **新增训练阶段**: 在原有的预训练(pt)、监督微调(sft)、奖励建模(rm)、PPO、DPO、KTO等训练阶段基础上，新增了 `action_cls` 训练阶段。

2. **ActionDecoder 模块**: 实现了一个轻量级的动作分类解码器，可以将模型在 `<ACTION>` 标记位置的隐藏状态映射到动作类别的logits。
   - 支持两种架构：
     - `linear`: 单层线性分类器
     - `mlp`: 两层MLP（含GELU激活函数）

3. **特殊标记 `<ACTION>`**: 系统会自动在tokenizer中注册该特殊标记，用于标识需要进行动作分类的位置。

4. **VideoEspresso 数据集支持**: 内置支持VideoEspresso数据集的三个训练阶段：
   - `VideoEspresso_stage1`: 基础视频理解
   - `VideoEspresso_stage2`: 中级视频任务
   - `VideoEspresso_stage3`: 高级动作分类

## 如何使用

### 1. 环境准备

确保已安装LlamaFactory及其依赖：

```bash
cd LlamaFactory
pip install -e .
```

### 2. 数据准备

#### 使用内置数据集

系统已经包含了VideoEspresso数据集的配置文件：
- `data/VideoEspresso_stage1.json`
- `data/VideoEspresso_stage2.json`
- `data/VideoEspresso_stage3.json`

#### 自定义数据集

在 `data/dataset_info.json` 中添加你的数据集配置：

```json
{
  "my_action_dataset": {
    "file_name": "my_action_dataset.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "videos": "videos"
    }
  }
}
```

数据格式示例 (`my_action_dataset.json`)：

```json
[
  {
    "conversations": [
      {
        "from": "human",
        "value": "<video>描述视频中的动作。<ACTION>"
      },
      {
        "from": "gpt",
        "value": "视频中的人正在跑步。"
      }
    ],
    "videos": ["path/to/video1.mp4"],
    "action_label": 0
  }
]
```

**注意**: 
- 在用户消息中添加 `<ACTION>` 标记，系统会在该位置提取特征用于分类
- `action_label` 字段表示动作类别的ID（从0开始）

### 3. 训练配置

创建训练配置文件 (例如 `action_cls_lora.yaml`)：

```yaml
### 模型配置
model_name_or_path: path/to/your/video-llm-model  # 例如：Qwen2-VL-7B

### 训练方法
stage: action_cls          # 使用动作分类训练阶段
do_train: true
finetuning_type: lora      # 可选：lora, full, freeze

### 数据集配置
dataset: VideoEspresso_stage3  # 或使用你自定义的数据集
cutoff_len: 8192
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16

### LoRA配置（如果使用lora）
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target: all

### 训练参数
output_dir: output/action_cls_model
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 1.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### 动作分类特定配置
num_action_classes: 51                    # 动作类别数量（例如HMDB-51为51，UCF-101为101）
action_decoder_type: linear               # 解码器类型：linear 或 mlp
action_decoder_hidden_size: null          # MLP隐藏层大小（仅当type=mlp时使用，null表示使用模型hidden_size）
action_decoder_path: null                 # 预训练的action decoder路径（可选）
action_token_lr_scale: 0.1                # <ACTION> token embedding的学习率缩放因子

### 日志配置
logging_steps: 10
save_steps: 500
save_total_limit: 2
plot_loss: true
```

### 4. 启动训练

使用命令行启动训练：

```bash
cd LlamaFactory

# 使用YAML配置文件
llamafactory-cli train action_cls_lora.yaml

# 或使用命令行参数
llamafactory-cli train \
    --stage action_cls \
    --model_name_or_path path/to/your/model \
    --dataset VideoEspresso_stage3 \
    --finetuning_type lora \
    --output_dir output/action_cls_model \
    --num_action_classes 51 \
    --action_decoder_type linear \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --learning_rate 1e-4 \
    --bf16
```

### 5. 分布式训练

使用多GPU训练：

```bash
# 使用 DeepSpeed
deepspeed --num_gpus 4 \
    src/train.py \
    --deepspeed examples/deepspeed/ds_z3_config.json \
    --stage action_cls \
    --model_name_or_path path/to/your/model \
    --dataset VideoEspresso_stage3 \
    --output_dir output/action_cls_model \
    --num_action_classes 51 \
    --finetuning_type lora \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --learning_rate 1e-4 \
    --bf16

# 使用 FSDP
accelerate launch --config_file examples/accelerate/fsdp_config.yaml \
    src/train.py \
    --stage action_cls \
    --model_name_or_path path/to/your/model \
    --dataset VideoEspresso_stage3 \
    --output_dir output/action_cls_model \
    --num_action_classes 51 \
    --finetuning_type lora \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --learning_rate 1e-4 \
    --bf16
```

### 6. 模型评估

```bash
llamafactory-cli train action_cls_lora.yaml \
    --do_train false \
    --do_eval true
```

### 7. 模型推理

训练完成后，模型会保存在指定的输出目录中，包含：
- 基础模型权重或LoRA适配器
- ActionDecoder权重 (`action_decoder.safetensors`)

加载和使用模型进行推理的示例代码：

```python
from llamafactory.model import load_model, load_tokenizer
from llamafactory.model.action_decoder import ActionDecoder
import torch

# 加载tokenizer和模型
tokenizer_module = load_tokenizer(model_args)
tokenizer = tokenizer_module["tokenizer"]
model = load_model(tokenizer, model_args, finetuning_args, is_trainable=False)

# 加载action decoder
action_decoder = ActionDecoder(
    hidden_size=model.config.hidden_size,
    num_classes=51,  # 与训练时一致
    decoder_type="linear"
)
action_decoder.load_pretrained("output/action_cls_model")
action_decoder.eval()

# 准备输入
prompt = "<video>这个视频中的动作是什么？<ACTION>"
inputs = tokenizer(prompt, return_tensors="pt")

# 前向传播
with torch.no_grad():
    outputs = model(**inputs, output_hidden_states=True)
    # 找到<ACTION> token的位置
    action_token_id = tokenizer.convert_tokens_to_ids("<ACTION>")
    action_positions = (inputs.input_ids == action_token_id).nonzero()
    # 提取对应的隐藏状态
    hidden_state = outputs.hidden_states[-1][action_positions[0][0], action_positions[0][1]]
    # 获取动作分类logits
    logits = action_decoder(hidden_state.unsqueeze(0))
    predicted_class = logits.argmax(dim=-1).item()

print(f"预测的动作类别: {predicted_class}")
```

## 参数说明

### 核心参数

| 参数名 | 类型 | 默认值 | 说明 |
|--------|------|--------|------|
| `stage` | str | - | 训练阶段，使用 `action_cls` |
| `num_action_classes` | int | 51 | 动作类别数量 |
| `action_decoder_type` | str | linear | 解码器类型：`linear` 或 `mlp` |
| `action_decoder_hidden_size` | int\|null | null | MLP隐藏层大小（仅mlp类型） |
| `action_decoder_path` | str\|null | null | 预训练解码器路径 |
| `action_token_lr_scale` | float | 0.1 | ACTION token学习率缩放 |

### 训练技巧

1. **学习率调整**: 
   - 建议对 `<ACTION>` token的embedding使用较小的学习率（通过 `action_token_lr_scale` 控制）
   - 基础学习率推荐范围：1e-5 到 1e-4

2. **批量大小**:
   - 视频数据通常较大，建议使用较小的batch size（1-4）配合梯度累积
   - 例如：`per_device_train_batch_size=2, gradient_accumulation_steps=4`

3. **解码器选择**:
   - 对于简单任务，使用 `linear` 解码器即可
   - 对于复杂动作分类，可以尝试 `mlp` 解码器并调整 `action_decoder_hidden_size`

4. **数据增强**:
   - 可以在数据预处理阶段添加视频增强（裁剪、翻转、色彩抖动等）

## 常见问题

### Q1: 如何确定 `num_action_classes` 的值？
A: 该值应该等于你的数据集中动作类别的总数。例如：
- HMDB-51: 51
- UCF-101: 101
- Kinetics-400: 400
- 自定义数据集：统计你数据中的唯一类别数

### Q2: `<ACTION>` 标记应该放在哪里？
A: 通常放在用户提问的末尾，表示需要在该位置进行动作分类。系统会提取该位置的隐藏状态进行分类。

### Q3: 是否可以微调整个模型？
A: 可以，通过设置 `finetuning_type: full` 即可。但由于视频模型通常较大，建议使用LoRA等参数高效方法。

### Q4: 如何使用自己的视频数据？
A: 按照上述"自定义数据集"部分的格式准备数据，确保：
- JSON格式正确
- 视频路径可访问
- 包含 `action_label` 字段
- 在消息中包含 `<ACTION>` 标记

### Q5: 训练时内存不足怎么办？
A: 可以尝试：
- 减小 `per_device_train_batch_size`
- 增大 `gradient_accumulation_steps`
- 使用 DeepSpeed ZeRO-3 进行显存优化
- 使用量化训练（QLoRA）
- 减小 `cutoff_len`

## 技术架构

### 工作流程

```
视频输入 → 视频编码器 → 大语言模型主干
                              ↓
                    提取<ACTION>位置的hidden state
                              ↓
                        ActionDecoder
                              ↓
                        动作类别logits
                              ↓
                    CrossEntropy Loss → 反向传播
```

### 关键组件

1. **ActionDecoder** (`LlamaFactory/src/llamafactory/model/action_decoder.py`):
   - 轻量级分类头
   - 支持linear和mlp两种架构
   - 可独立保存和加载

2. **ActionClassificationTrainer** (`LlamaFactory/src/llamafactory/train/action_cls/trainer.py`):
   - 继承自HuggingFace Trainer
   - 自定义损失计算
   - 管理ActionDecoder的训练和保存

3. **数据处理器** (`LlamaFactory/src/llamafactory/data/processor/action_cls.py`):
   - 处理视频数据和动作标签
   - 识别和处理 `<ACTION>` 标记

## 示例数据集

仓库中包含了以下示例数据集：

- `test_BGEM3.py`: BGE-M3嵌入模型测试脚本
- `test_gsm8k.py`: GSM8K数学推理数据集加载示例
- `LlamaFactory/data/mllm_video_demo.json`: 视频问答示例
- `LlamaFactory/data/VideoEspresso_stage*.json`: VideoEspresso数据集

## 相关链接

- LlamaFactory GitHub: https://github.com/hiyouga/LLaMA-Factory
- LlamaFactory 文档: https://llamafactory.readthedocs.io/

## 更新日志

- **2026-02-09**: 添加 `action_cls` 训练阶段，支持视频动作分类
  - 新增 ActionDecoder 模块
  - 支持 `<ACTION>` 特殊标记
  - 集成 VideoEspresso 数据集
  - 提供完整的训练和推理工具链

---

如有问题或建议，请提交 Issue 或 Pull Request。
