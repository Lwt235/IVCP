[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                               | 0/502 [00:00<?, ?it/s][libav.opus|ERROR]Error parsing Opus packet header.
Unsloth: Will smartly offload gradients to save VRAM!
  2%|â–ˆâ–ˆâ–ˆâ–                                                                                                                                            | 12/502 [1:01:06<39:12:59, 288.12s/it]
{'loss': 3.2118, 'grad_norm': 2.664071559906006, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 3.2551, 'grad_norm': 2.5179355144500732, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 3.1276, 'grad_norm': 2.4338877201080322, 'learning_rate': 8e-05, 'epoch': 0.01}
{'loss': 2.9534, 'grad_norm': 2.1510915756225586, 'learning_rate': 0.00012, 'epoch': 0.01}
{'loss': 2.7625, 'grad_norm': 1.8276110887527466, 'learning_rate': 0.00016, 'epoch': 0.01}
{'loss': 2.6162, 'grad_norm': 1.8280491828918457, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 2.4362, 'grad_norm': 1.3995815515518188, 'learning_rate': 0.00019959758551307849, 'epoch': 0.01}
{'loss': 2.2907, 'grad_norm': 1.219079613685608, 'learning_rate': 0.00019919517102615693, 'epoch': 0.02}
{'loss': 2.299, 'grad_norm': 0.8139461874961853, 'learning_rate': 0.0001987927565392354, 'epoch': 0.02}
{'loss': 2.1375, 'grad_norm': 0.8515397310256958, 'learning_rate': 0.00019839034205231388, 'epoch': 0.02}
{'loss': 2.1015, 'grad_norm': 0.6910043358802795, 'learning_rate': 0.00019798792756539236, 'epoch': 0.02}
{'loss': 2.0295, 'grad_norm': 0.6793503165245056, 'learning_rate': 0.00019758551307847084, 'epoch': 0.02}
