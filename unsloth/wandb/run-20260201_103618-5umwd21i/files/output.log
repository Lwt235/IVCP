[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                               | 0/395 [00:00<?, ?it/s][libav.opus|ERROR]Error parsing Opus packet header.
Unsloth: Will smartly offload gradients to save VRAM!
  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                    | 31/395 [1:36:31<18:42:17, 184.99s/it]Traceback (most recent call last):
{'loss': 3.0835, 'grad_norm': 1.3564155101776123, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 3.0864, 'grad_norm': 1.3217952251434326, 'learning_rate': 4e-05, 'epoch': 0.01}
{'loss': 3.2164, 'grad_norm': 1.3486785888671875, 'learning_rate': 8e-05, 'epoch': 0.01}
{'loss': 3.0708, 'grad_norm': 1.3453991413116455, 'learning_rate': 0.00012, 'epoch': 0.01}
{'loss': 2.8609, 'grad_norm': 1.1780387163162231, 'learning_rate': 0.00016, 'epoch': 0.01}
{'loss': 2.6298, 'grad_norm': 1.1788041591644287, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 2.4875, 'grad_norm': 1.0524171590805054, 'learning_rate': 0.0001994871794871795, 'epoch': 0.02}
{'loss': 2.2766, 'grad_norm': 1.0127531290054321, 'learning_rate': 0.00019897435897435898, 'epoch': 0.02}
{'loss': 2.2586, 'grad_norm': 1.017912745475769, 'learning_rate': 0.00019846153846153847, 'epoch': 0.02}
{'loss': 2.1689, 'grad_norm': 0.9061760306358337, 'learning_rate': 0.00019794871794871796, 'epoch': 0.03}
{'loss': 1.9968, 'grad_norm': 0.6826383471488953, 'learning_rate': 0.00019743589743589744, 'epoch': 0.03}
{'loss': 1.8797, 'grad_norm': 0.6630290150642395, 'learning_rate': 0.00019692307692307696, 'epoch': 0.03}
{'loss': 1.9624, 'grad_norm': 0.5753082036972046, 'learning_rate': 0.00019641025641025642, 'epoch': 0.03}
{'loss': 1.8753, 'grad_norm': 0.5508617162704468, 'learning_rate': 0.0001958974358974359, 'epoch': 0.04}
{'loss': 1.8566, 'grad_norm': 0.5073513984680176, 'learning_rate': 0.0001953846153846154, 'epoch': 0.04}
{'loss': 1.751, 'grad_norm': 0.48518532514572144, 'learning_rate': 0.00019487179487179487, 'epoch': 0.04}
{'loss': 1.8194, 'grad_norm': 0.4735018312931061, 'learning_rate': 0.00019435897435897436, 'epoch': 0.04}
{'loss': 1.8069, 'grad_norm': 0.5815670490264893, 'learning_rate': 0.00019384615384615385, 'epoch': 0.05}
{'loss': 1.8225, 'grad_norm': 0.531268298625946, 'learning_rate': 0.00019333333333333333, 'epoch': 0.05}
{'loss': 1.7827, 'grad_norm': 0.6872534155845642, 'learning_rate': 0.00019282051282051282, 'epoch': 0.05}
{'loss': 1.6539, 'grad_norm': 0.4731380343437195, 'learning_rate': 0.00019230769230769233, 'epoch': 0.05}
{'loss': 1.7222, 'grad_norm': 0.4221855700016022, 'learning_rate': 0.00019179487179487182, 'epoch': 0.06}
{'loss': 1.7083, 'grad_norm': 0.4609168767929077, 'learning_rate': 0.0001912820512820513, 'epoch': 0.06}
{'loss': 1.6447, 'grad_norm': 0.47832900285720825, 'learning_rate': 0.0001907692307692308, 'epoch': 0.06}
{'loss': 1.7022, 'grad_norm': 0.5103411078453064, 'learning_rate': 0.00019025641025641025, 'epoch': 0.06}
{'loss': 1.7555, 'grad_norm': 0.4295034110546112, 'learning_rate': 0.00018974358974358974, 'epoch': 0.07}
{'loss': 1.6529, 'grad_norm': 0.4463515877723694, 'learning_rate': 0.00018923076923076923, 'epoch': 0.07}
{'loss': 1.7155, 'grad_norm': 0.5087009072303772, 'learning_rate': 0.0001887179487179487, 'epoch': 0.07}
{'loss': 1.7186, 'grad_norm': 0.4495973587036133, 'learning_rate': 0.0001882051282051282, 'epoch': 0.07}
{'loss': 1.589, 'grad_norm': 0.41322630643844604, 'learning_rate': 0.0001876923076923077, 'epoch': 0.08}
{'loss': 1.6486, 'grad_norm': 0.39759594202041626, 'learning_rate': 0.0001871794871794872, 'epoch': 0.08}
  File "/home/liwt/IVCP/unsloth/model.py", line 66, in <module>
    dataloader_prefetch_factor = 2,
    ^^^^^^^^^^^^^^^
  File "/home/liwt/IVCP/unsloth/unsloth_compiled_cache/UnslothSFTTrainer.py", line 55, in wrapper
    output = f(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 272, in _fast_inner_training_loop
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/loss_utils.py", line 326, in _unsloth_get_batch_samples
    batch_samples += [next(epoch_iterator)]
                      ^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/accelerate/data_loader.py", line 579, in __iter__
    next_batch = next(dataloader_iter)
                 ^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 812, in __call__
    image, video, video_kwarg = self._extract_images_videos_for_example(example, messages)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 911, in _extract_images_videos_for_example
    image, video, video_kwarg = process_vision_info(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 570, in process_vision_info
    video_input, video_sample_fps = fetch_video(vision_info, image_factor=size_factor, return_video_sample_fps=True)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 469, in fetch_video
    video, sample_fps = VIDEO_READER_BACKENDS[video_reader_backend](ele)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 261, in _read_video_torchvision
    video, audio, info = io.read_video(
                         ^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torchvision/io/video.py", line 339, in read_video
    video_frames = _read_from_stream(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torchvision/io/video.py", line 236, in _read_from_stream
    if frame.pts >= end_offset:
       ^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/liwt/IVCP/unsloth/model.py", line 66, in <module>
    dataloader_prefetch_factor = 2,
^^^^^^^^^^^
  File "/home/liwt/IVCP/unsloth/unsloth_compiled_cache/UnslothSFTTrainer.py", line 55, in wrapper
    output = f(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 272, in _fast_inner_training_loop
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/loss_utils.py", line 326, in _unsloth_get_batch_samples
    batch_samples += [next(epoch_iterator)]
                      ^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/accelerate/data_loader.py", line 579, in __iter__
    next_batch = next(dataloader_iter)
                 ^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 812, in __call__
    image, video, video_kwarg = self._extract_images_videos_for_example(example, messages)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 911, in _extract_images_videos_for_example
    image, video, video_kwarg = process_vision_info(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 570, in process_vision_info
    video_input, video_sample_fps = fetch_video(vision_info, image_factor=size_factor, return_video_sample_fps=True)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 469, in fetch_video
    video, sample_fps = VIDEO_READER_BACKENDS[video_reader_backend](ele)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/vision_utils.py", line 261, in _read_video_torchvision
    video, audio, info = io.read_video(
                         ^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torchvision/io/video.py", line 339, in read_video
    video_frames = _read_from_stream(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/liwt/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torchvision/io/video.py", line 236, in _read_from_stream
    if frame.pts >= end_offset:
       ^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
